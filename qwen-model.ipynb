{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13696630,"sourceType":"datasetVersion","datasetId":8712215}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# !pip install --upgrade pip\n# !pip install unsloth peft trl wandb\n# !pip install -q human-eval datasets transformers accelerate einops tiktoken tqdm","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os, sys, json, math, time, random, tempfile, shutil, subprocess, signal, resource, textwrap, re\nfrom dataclasses import dataclass\nfrom pathlib import Path\nfrom typing import List, Dict, Any, Optional\nfrom kaggle_secrets import UserSecretsClient\nimport torch\nfrom datasets import load_dataset, Dataset\nfrom transformers import AutoTokenizer\nfrom unsloth import FastLanguageModel, is_bfloat16_supported\nfrom trl import SFTTrainer\nimport trl, peft\nfrom peft import LoraConfig\nfrom transformers import TrainerCallback\n# W&B (tùy chọn)\nimport wandb","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(\"Device:\", DEVICE)\n\ndef set_seed(seed: int = 42):\n    random.seed(seed); torch.manual_seed(seed); \n    if torch.cuda.is_available(): torch.cuda.manual_seed_all(seed)\nset_seed(42)\n\n# ---- Helper: loại bỏ ```python ... ``` khi model sinh ra\n_CODE_FENCE_RE = re.compile(r\"```(?:python)?\\n(.*?)```\", re.DOTALL | re.IGNORECASE)\ndef extract_code(text: str) -> str:\n    m = _CODE_FENCE_RE.search(text)\n    if m:\n        return m.group(1).strip()\n    # fallback: bỏ các lời giải thích, chỉ lấy từ dòng chứa 'def ' đầu tiên\n    lines = text.splitlines()\n    for i, ln in enumerate(lines):\n        if ln.strip().startswith(\"def \"):\n            return \"\\n\".join(lines[i:]).strip()\n    return text.strip()\n\n# ---- Sandbox executor (POSIX): chạy test trong tiến trình con, giới hạn thời gian & tài nguyên\ndef _write(path: Path, content: str):\n    path.write_text(content, encoding=\"utf-8\")\n\ndef _limit_resources(timeout_sec: int = 5, max_mem_gb: int = 2):\n    try:\n        resource.setrlimit(resource.RLIMIT_CPU, (timeout_sec, timeout_sec))\n    except Exception:\n        pass\n    try:\n        bytes_lim = max_mem_gb * 1024**3\n        resource.setrlimit(resource.RLIMIT_AS, (bytes_lim, bytes_lim))\n    except Exception:\n        pass\n    try:\n        resource.setrlimit(resource.RLIMIT_FSIZE, (10 * 1024**2, 10 * 1024**2))  # 10MB\n    except Exception:\n        pass\n\nEVAL_TEMPLATE = \"\"\"\nimport importlib.util, sys, traceback, json\n\n# Nạp module ứng viên (chứa lời giải)\nspec = importlib.util.spec_from_file_location(\"candidate\", \"candidate.py\")\ncand = importlib.util.module_from_spec(spec)\nspec.loader.exec_module(cand)  # có thể raise lỗi syntax/exception\n\n# Dán test code từ dataset (định nghĩa check(...))\n{test_code}\n\npassed = None\ntry:\n    candidate = getattr(cand, \"{entry_point}\")\n    # Một số test dùng assert và không trả về; nếu không exception thì coi là passed\n    r = check(candidate)\n    passed = (r is None) or bool(r)\nexcept Exception as e:\n    # Nếu test ném lỗi -> fail\n    sys.stderr.write(\"TEST_ERROR: \" + str(e) + \"\\n\")\n    passed = False\n\nprint(json.dumps({{\"passed\": bool(passed)}}))\n\"\"\"\n\ndef run_program_with_tests(program: str, test_code: str, entry_point: str, timeout_sec: int = 6) -> bool:\n    work = Path(tempfile.mkdtemp(prefix=\"lcd_eval_\"))\n    try:\n        cand = work / \"candidate.py\"\n        _write(cand, program)\n        runner = work / \"run_eval.py\"\n        _write(runner, EVAL_TEMPLATE.format(test_code=test_code, entry_point=entry_point))\n\n        # Thực thi trong tiến trình con với giới hạn\n        cmd = [sys.executable, str(runner)]\n        proc = subprocess.Popen(\n            cmd,\n            cwd=str(work),\n            stdout=subprocess.PIPE,\n            stderr=subprocess.PIPE,\n            text=True,\n            preexec_fn=lambda: _limit_resources(timeout_sec=timeout_sec, max_mem_gb=2),\n        )\n        try:\n            out, err = proc.communicate(timeout=timeout_sec + 1)\n        except subprocess.TimeoutExpired:\n            proc.kill()\n            return False\n\n        if proc.returncode != 0:\n            return False\n\n        try:\n            obj = json.loads(out.strip().splitlines()[-1])\n            return bool(obj.get(\"passed\", False))\n        except Exception:\n            return False\n    finally:\n        shutil.rmtree(work, ignore_errors=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ==== Cấu hình model & dữ liệu ====\nMODEL_NAME = \"Qwen/Qwen2.5-Coder-3B-Instruct\"\nMAX_SEQ_LEN = 3072\n\n# Đường dẫn 2 file JSONL (cập nhật theo máy của bạn)\nTRAIN_JSONL = \"/kaggle/input/datasetleetcode/LeetCodeDataset-train.jsonl\"\nTEST_JSONL  = \"/kaggle/input/datasetleetcode/LeetCodeDataset-test.jsonl\"\n\n# Tham số sinh mã\nGEN_KW = dict(\n    max_new_tokens=640,\n    temperature=0.2,\n    top_p=0.95,\n    do_sample=True,\n)\nSEED = 42\n\nset_seed(SEED)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ==== Load model & tokenizer (QLoRA 4-bit) ====\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    MODEL_NAME,\n    max_seq_length=MAX_SEQ_LEN,\n    dtype=None,\n    load_in_4bit=True,\n)\nprint(\"Loaded:\", MODEL_NAME)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from datasets import load_dataset\n\ntrain_ds = load_dataset(\"json\", data_files=TRAIN_JSONL, split=\"train\")\ntest_ds  = load_dataset(\"json\", data_files=TEST_JSONL,  split=\"train\")\n\nREQUIRED_FIELDS = [\"task_id\", \"problem_description\", \"completion\", \"starter_code\", \"test\", \"entry_point\"]\nfor name, ds in [(\"train\", train_ds), (\"test\", test_ds)]:\n    missing = [c for c in REQUIRED_FIELDS if c not in ds.column_names]\n    if missing:\n        print(f\"[CẢNH BÁO] {name} thiếu các cột: {missing}\")\n\nSYSTEM_INST = (\n    \"You are an expert competitive programming assistant. \"\n    \"Given a problem description and optional starter code, write a complete and correct Python solution. \"\n    \"Only output Python code, no explanations or markdown.\"\n)\n\ndef map_to_sft(example):\n    desc = (example.get(\"problem_description\") or \"\").strip()\n    starter = (example.get(\"starter_code\") or \"\").strip()\n    sol = (example.get(\"completion\") or \"\").strip()\n    if not desc or not sol:\n        return {\"text\": None}\n    user_prompt = desc\n    if starter:\n        user_prompt += f\"\\n\\nStarter Code:\\n{starter}\"\n    messages = [\n        {\"role\": \"system\", \"content\": SYSTEM_INST},\n        {\"role\": \"user\", \"content\": user_prompt},\n        {\"role\": \"assistant\", \"content\": sol},\n    ]\n    txt = tokenizer.apply_chat_template(messages, tokenize=False) + tokenizer.eos_token\n    return {\"text\": txt}\n\ntrain_sft = train_ds.map(map_to_sft, num_proc=4, remove_columns=[c for c in train_ds.column_names if c!=\"task_id\"])\ntrain_sft = train_sft.filter(lambda ex: ex[\"text\"] is not None)\n\nprint(\"Train samples:\", len(train_sft))\nprint(\"Test samples:\", len(test_ds))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ==== Cấu hình LoRA ====\nlora_cfg = LoraConfig(\n    r=32,\n    lora_alpha=64,\n    lora_dropout=0.0,           # 0.0 thường tốt cho QLoRA\n    bias=\"none\",\n    target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"],\n    task_type=\"CAUSAL_LM\",\n)\n\nmodel = FastLanguageModel.get_peft_model(\n    model,\n    r=lora_cfg.r,\n    target_modules=lora_cfg.target_modules,\n    lora_alpha=lora_cfg.lora_alpha,\n    lora_dropout=lora_cfg.lora_dropout,\n    bias=lora_cfg.bias,\n    use_gradient_checkpointing=\"unsloth\",\n    random_state=SEED,\n    use_rslora=True,\n    loftq_config=None,\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# WandB setup - Giữ nguyên\nuser_secrets = UserSecretsClient()\nsecret_value_0 = user_secrets.get_secret(\"wandb\")\nwandb.login(key=secret_value_0)\nwandb.init(project=\"UET deeplearning\", name=\"qwen-leetcode\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ==== Self-Check Callback (đã chỉnh đúng fields) ====\nclass SelfCheckCallback(TrainerCallback):\n    def __init__(self, tokenizer, test_ds, sample_train=16, sample_test=32):\n        self.tok = tokenizer\n        self.test_ds = test_ds\n        self.sample_train = sample_train\n        self.sample_test = sample_test\n\n    def _gen_completion(self, model, problem_description: str, starter_code: str = \"\") -> str:\n        # Dùng problem_description + starter_code làm input\n        user_prompt = problem_description.strip()\n        if starter_code:\n            user_prompt += f\"\\n\\nStarter Code:\\n{starter_code.strip()}\"\n        msgs = [\n            {\"role\": \"system\", \"content\": SYSTEM_INST},\n            {\"role\": \"user\", \"content\": user_prompt},\n        ]\n        prompt_text = self.tok.apply_chat_template(msgs, tokenize=False, add_generation_prompt=True)\n        enc = self.tok(prompt_text, return_tensors=\"pt\").to(model.device)\n        FastLanguageModel.for_inference(model)\n        with torch.no_grad():\n            out = model.generate(**enc, **GEN_KW)\n        gen_text = self.tok.decode(out[0][enc[\"input_ids\"].shape[1]:], skip_special_tokens=True)\n        return extract_code(gen_text)\n\n    def _check_many(self, model, dataset, n=16):\n        n = min(n, len(dataset))\n        if n <= 0:\n            return 0.0\n        idxs = random.sample(range(len(dataset)), n)\n        ok = 0\n        for i in idxs:\n            ex = dataset[i]\n            try:\n                # Sinh code\n                completion = self._gen_completion(\n                    model,\n                    ex.get(\"problem_description\", \"\"),\n                    ex.get(\"starter_code\", \"\")\n                )\n                # Gộp starter_code (nếu có) + completion\n                program = (ex.get(\"starter_code\",\"\") or \"\") + \"\\n\" + completion\n                passed = run_program_with_tests(\n                    program,\n                    ex.get(\"test\",\"\") or \"\",\n                    ex.get(\"entry_point\",\"\")\n                )\n                ok += int(bool(passed))\n            except Exception as e:\n                pass\n        return ok / n\n\n    def on_epoch_end(self, args, state, control, **kwargs):\n        model = kwargs.get(\"model\")\n        try:\n            train_rate = self._check_many(model, train_ds, n=self.sample_train)\n            test_rate  = self._check_many(model, self.test_ds, n=self.sample_test)\n            print(f\"[Self-Check] epoch={state.epoch:.2f}  train_pass@1={train_rate:.3f}  test_pass@1={test_rate:.3f}\")\n            if USE_WANDB:\n                wandb.log({\n                    \"selfcheck/train_pass@1\": train_rate,\n                    \"selfcheck/test_pass@1\": test_rate,\n                    \"epoch\": state.epoch\n                })\n        except Exception as e:\n            print(\"[Self-Check] Error:\", e)\n        return control\n\n# Khởi tạo callback\nselfcheck_cb = SelfCheckCallback(tokenizer, test_ds, sample_train=8, sample_test=16)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ==== Trainer ====\nOUTPUT_DIR = \"/kaggle/working/qwen2_5_leetcode_lora\"\n\ntrainer = SFTTrainer(\n    model=model,\n    tokenizer=tokenizer,\n    train_dataset=train_sft,\n    eval_dataset=None,                  \n    dataset_text_field=\"text\",\n    max_seq_length=MAX_SEQ_LEN,\n    args=trl.SFTConfig(\n        per_device_train_batch_size=2,\n        gradient_accumulation_steps=16,     \n        num_train_epochs=3,\n        warmup_ratio=0.06,\n        learning_rate=4e-5,                 \n        lr_scheduler_type=\"cosine\",\n        weight_decay=0.05,\n        logging_steps=10,\n        save_steps=500,\n        save_total_limit=2,\n        output_dir=OUTPUT_DIR,\n        gradient_checkpointing=True,\n        bf16=is_bfloat16_supported(),\n        fp16=not is_bfloat16_supported(),\n        optim=\"adamw_torch\",\n        report_to=\"wandb\",\n    ),\n)\n\n# Gắn self-check callback\ntrainer.add_callback(selfcheck_cb)\n\nprint(\"Bắt đầu train ...\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_result = trainer.train()\nprint(\"Hoàn tất train!\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ==== Lưu adapter & tokenizer ====\ntrainer.model.save_pretrained(OUTPUT_DIR)\ntokenizer.save_pretrained(OUTPUT_DIR)\n\n#(Tuỳ chọn) Merge LoRA vào base model (nếu đủ VRAM)\n#Cảnh báo: merge 16-bit có thể tốn nhiều bộ nhớ!\nFastLanguageModel.for_inference(model)\nmodel.save_pretrained_merged(f\"{OUTPUT_DIR}_merged\", tokenizer, save_method=\"merged_16bit\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ==== Đánh giá pass@1 trên test ====\nimport tqdm\n\ndef predict_and_score(model, tok, dataset, out_path=\"/kaggle/working/predictions.jsonl\", max_samples=None):\n    FastLanguageModel.for_inference(model)\n    total = len(dataset) if max_samples is None else min(max_samples, len(dataset))\n    ok = 0\n    with open(out_path, \"w\", encoding=\"utf-8\") as f:\n        for i in range(total):\n            ex = dataset[i]\n            # Sinh lời giải\n            msgs = [\n                {\"role\": \"system\", \"content\": SYSTEM_INST},\n                {\"role\": \"user\", \"content\": ex[\"query\"]},\n            ]\n            prompt_text = tok.apply_chat_template(msgs, tokenize=False, add_generation_prompt=True)\n            enc = tok(prompt_text, return_tensors=\"pt\").to(model.device)\n            out = model.generate(**enc, **GEN_KW)\n            gen_text = tok.decode(out[0][enc[\"input_ids\"].shape[1]:], skip_special_tokens=True)\n            completion = extract_code(gen_text)\n\n            # Lưu dạng {task_id, completion} — tương thích eval_lcd\n            f.write(json.dumps({\"task_id\": ex[\"task_id\"], \"completion\": completion}) + \"\\n\")\n\n            # Tự chấm qua test code\n            program = (ex.get(\"prompt\",\"\") or \"\") + \"\\n\" + completion\n            if run_program_with_tests(program, ex.get(\"test\",\"\") or \"\", ex.get(\"entry_point\",\"\")):\n                ok += 1\n\n    return {\"total\": total, \"pass@1\": ok/total if total else 0.0}\n\n# Load lại model (nếu cần)\nFastLanguageModel.for_inference(model)\nscore = predict_and_score(model, tokenizer, test_ds, out_path=\"/kaggle/working/lcd_predictions.jsonl\", max_samples=None)\nprint(\"Test pass@1:\", score)\n# File dự đoán cho eval_lcd CLI:\nprint(\"Predictions saved to lcd_predictions.jsonl\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}